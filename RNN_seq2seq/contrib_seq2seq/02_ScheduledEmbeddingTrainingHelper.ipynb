{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To plot learning curve graph\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for pretty print\n",
    "from pprint import pprint\n",
    "\n",
    "# for tokenizer\n",
    "import re\n",
    "\n",
    "# for word counter in vocabulary dictionary\n",
    "from collections import Counter\n",
    "\n",
    "# for checkpoint paths\n",
    "import os\n",
    "\n",
    "# for fancy progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# for output_projection\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# for sampling probability decay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maximum length of input and target sentences including paddings\n",
    "enc_sentence_length = 10\n",
    "dec_sentence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch_size: 2\n",
    "input_batches = [\n",
    "    ['Hi What is your name?', 'Nice to meet you!'],\n",
    "    ['Which programming language do you use?', 'See you later.'],\n",
    "    ['Where do you live?', 'What is your major?'],\n",
    "    ['What do you want to drink?', 'What is your favorite beer?']]\n",
    "\n",
    "target_batches = [\n",
    "    ['Hi this is Jaemin.', 'Nice to meet you too!'],\n",
    "    ['I like Python.', 'Bye Bye.'],\n",
    "    ['I live in Seoul, South Korea.', 'I study industrial engineering.'],\n",
    "    ['Beer please!', 'Leffe brown!']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi What is your name?',\n",
       " 'Nice to meet you!',\n",
       " 'Which programming language do you use?',\n",
       " 'See you later.',\n",
       " 'Where do you live?',\n",
       " 'What is your major?',\n",
       " 'What do you want to drink?',\n",
       " 'What is your favorite beer?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_sentences = []\n",
    "for input_batch in input_batches:\n",
    "    all_input_sentences.extend(input_batch)\n",
    "    \n",
    "all_target_sentences = []\n",
    "for target_batch in target_batches:\n",
    "    all_target_sentences.extend(target_batch)\n",
    "\n",
    "# Example\n",
    "all_input_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '?', '?', '\"', 'sdfs', '%', '@', '#', '%']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(sentence):\n",
    "    tokens = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "    return tokens\n",
    "\n",
    "# Example\n",
    "tokenizer('Hello world?? \"sdfs%@#%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'!': 12,\n",
      "  '.': 19,\n",
      "  '?': 1,\n",
      "  'Hi': 8,\n",
      "  'Nice': 10,\n",
      "  'See': 17,\n",
      "  'What': 3,\n",
      "  'Where': 20,\n",
      "  'Which': 13,\n",
      "  '_PAD': 0,\n",
      "  'beer': 26,\n",
      "  'do': 6,\n",
      "  'drink': 24,\n",
      "  'favorite': 25,\n",
      "  'is': 4,\n",
      "  'language': 15,\n",
      "  'later': 18,\n",
      "  'live': 21,\n",
      "  'major': 22,\n",
      "  'meet': 11,\n",
      "  'name': 9,\n",
      "  'programming': 14,\n",
      "  'to': 7,\n",
      "  'use': 16,\n",
      "  'want': 23,\n",
      "  'you': 2,\n",
      "  'your': 5},\n",
      " {0: '_PAD',\n",
      "  1: '?',\n",
      "  2: 'you',\n",
      "  3: 'What',\n",
      "  4: 'is',\n",
      "  5: 'your',\n",
      "  6: 'do',\n",
      "  7: 'to',\n",
      "  8: 'Hi',\n",
      "  9: 'name',\n",
      "  10: 'Nice',\n",
      "  11: 'meet',\n",
      "  12: '!',\n",
      "  13: 'Which',\n",
      "  14: 'programming',\n",
      "  15: 'language',\n",
      "  16: 'use',\n",
      "  17: 'See',\n",
      "  18: 'later',\n",
      "  19: '.',\n",
      "  20: 'Where',\n",
      "  21: 'live',\n",
      "  22: 'major',\n",
      "  23: 'want',\n",
      "  24: 'drink',\n",
      "  25: 'favorite',\n",
      "  26: 'beer'},\n",
      " 26)\n",
      "\n",
      "\n",
      "({'!': 2,\n",
      "  ',': 19,\n",
      "  '.': 1,\n",
      "  'Beer': 25,\n",
      "  'Bye': 4,\n",
      "  'Hi': 5,\n",
      "  'I': 3,\n",
      "  'Jaemin': 8,\n",
      "  'Korea': 21,\n",
      "  'Leffe': 27,\n",
      "  'Nice': 9,\n",
      "  'Python': 15,\n",
      "  'Seoul': 18,\n",
      "  'South': 20,\n",
      "  '_PAD': 0,\n",
      "  'brown': 28,\n",
      "  'engineering': 24,\n",
      "  'in': 17,\n",
      "  'industrial': 23,\n",
      "  'is': 7,\n",
      "  'like': 14,\n",
      "  'live': 16,\n",
      "  'meet': 11,\n",
      "  'please': 26,\n",
      "  'study': 22,\n",
      "  'this': 6,\n",
      "  'to': 10,\n",
      "  'too': 13,\n",
      "  'you': 12},\n",
      " {0: '_PAD',\n",
      "  1: '.',\n",
      "  2: '!',\n",
      "  3: 'I',\n",
      "  4: 'Bye',\n",
      "  5: 'Hi',\n",
      "  6: 'this',\n",
      "  7: 'is',\n",
      "  8: 'Jaemin',\n",
      "  9: 'Nice',\n",
      "  10: 'to',\n",
      "  11: 'meet',\n",
      "  12: 'you',\n",
      "  13: 'too',\n",
      "  14: 'like',\n",
      "  15: 'Python',\n",
      "  16: 'live',\n",
      "  17: 'in',\n",
      "  18: 'Seoul',\n",
      "  19: ',',\n",
      "  20: 'South',\n",
      "  21: 'Korea',\n",
      "  22: 'study',\n",
      "  23: 'industrial',\n",
      "  24: 'engineering',\n",
      "  25: 'Beer',\n",
      "  26: 'please',\n",
      "  27: 'Leffe',\n",
      "  28: 'brown'},\n",
      " 28)\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences, is_target=False, max_vocab_size=None):\n",
    "    word_counter = Counter()\n",
    "    vocab = dict()\n",
    "    reverse_vocab = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        word_counter.update(tokens)\n",
    "        \n",
    "    if max_vocab_size is None:\n",
    "        max_vocab_size = len(word_counter)\n",
    "    \n",
    "    if is_target:\n",
    "        vocab['_GO'] = 0\n",
    "        vocab['_PAD'] = 1\n",
    "        vocab_idx = 2\n",
    "        for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "    else:\n",
    "        vocab['_PAD'] = 0\n",
    "        vocab_idx = 1\n",
    "        for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "            \n",
    "    for key, value in vocab.items():\n",
    "        reverse_vocab[value] = key\n",
    "            \n",
    "    return vocab, reverse_vocab, max_vocab_size\n",
    "\n",
    "# Example\n",
    "pprint(build_vocab(all_input_sentences))\n",
    "print('\\n')\n",
    "pprint(build_vocab(all_target_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocabulary size: 26\n",
      "target vocabulary size: 28\n"
     ]
    }
   ],
   "source": [
    "enc_vocab, enc_reverse_vocab, enc_vocab_size = build_vocab(all_input_sentences)\n",
    "dec_vocab, dec_reverse_vocab, dec_vocab_size = build_vocab(all_target_sentences, is_target=True)\n",
    "\n",
    "print('input vocabulary size:', enc_vocab_size)\n",
    "print('target vocabulary size:', dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice 10\n",
      "to 7\n",
      "meet 11\n",
      "you 2\n",
      "! 12\n"
     ]
    }
   ],
   "source": [
    "def token2idx(word, vocab):\n",
    "    return vocab[word]\n",
    "\n",
    "for token in tokenizer('Nice to meet you!'):\n",
    "    print(token, token2idx(token, enc_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi What is your name?\n",
      "([8, 3, 4, 5, 9, 1, 0, 0, 0, 0], 6)\n",
      "Hi this is Jaemin.\n",
      "([0, 6, 7, 8, 9, 2, 1, 1, 1, 1, 1], 5)\n"
     ]
    }
   ],
   "source": [
    "def sent2idx(sent, vocab=enc_vocab, max_sentence_length=enc_sentence_length, is_target=False):\n",
    "    tokens = tokenizer(sent)\n",
    "    current_length = len(tokens)\n",
    "    pad_length = max_sentence_length - current_length\n",
    "    if is_target:\n",
    "        return [0] + [token2idx(token, vocab) for token in tokens] + [1] * pad_length, current_length\n",
    "    else:\n",
    "        return [token2idx(token, vocab) for token in tokens] + [0] * pad_length, current_length\n",
    "\n",
    "# Enc Example\n",
    "print('Hi What is your name?')\n",
    "print(sent2idx('Hi What is your name?'))\n",
    "\n",
    "# Dec Example\n",
    "print('Hi this is Jaemin.')\n",
    "print(sent2idx('Hi this is Jaemin.', vocab=dec_vocab, max_sentence_length=dec_sentence_length, is_target=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx2token(idx, reverse_vocab):\n",
    "    return reverse_vocab[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_vocab=dec_reverse_vocab):\n",
    "    return \" \".join([idx2token(idx, reverse_vocab) for idx in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters / Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoConfig:\n",
    "    \n",
    "    # Model\n",
    "    hidden_size = 30\n",
    "    enc_emb_size = 30\n",
    "    dec_emb_size = 30\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell\n",
    "    \n",
    "    # Training\n",
    "    optimizer = tf.train.RMSPropOptimizer\n",
    "    n_epoch = 801\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Sampling Probability\n",
    "    # with decay => 'Curriculumn Learning'\n",
    "    sampling_probability_list = np.linspace(\n",
    "        start=0.0,\n",
    "        stop=1.0,\n",
    "        num=n_epoch,\n",
    "        dtype=np.float32)\n",
    "\n",
    "    # Checkpoint path\n",
    "    ckpt_dir = './ckpt_dir/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, config, mode='training'):\n",
    "        assert mode in ['training', 'evaluation', 'inference']\n",
    "        self.mode = mode\n",
    "\n",
    "        # Model\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.enc_emb_size = config.enc_emb_size\n",
    "        self.dec_emb_size = config.dec_emb_size\n",
    "        self.cell = config.cell\n",
    "        \n",
    "        # Training\n",
    "        self.optimizer = config.optimizer\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.learning_rate = config.learning_rate\n",
    "        \n",
    "        # Sampling Probability\n",
    "        self.sampling_probability_list = config.sampling_probability_list\n",
    "        \n",
    "        # Checkpoint path\n",
    "        self.ckpt_dir = config.ckpt_dir\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        self.enc_inputs = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[None, enc_sentence_length],\n",
    "            name='input_sentences')\n",
    "\n",
    "        self.enc_sequence_length = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[None,],\n",
    "            name='input_sequence_length')\n",
    "        \n",
    "        if self.mode == 'training':\n",
    "            self.dec_inputs = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None, dec_sentence_length+1],\n",
    "                name='target_sentences')\n",
    "\n",
    "            self.dec_sequence_length = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None,],\n",
    "                name='target_sequence_length')\n",
    "\n",
    "            self.sampling_probability = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=[],\n",
    "                name='sampling_probability')\n",
    "            # 0.0 ≤ sampling_probability ≤ 1.0\n",
    "            # 0.0: no sampling => `ScheduledEmbedidngTrainingHelper` is equivalent to `TrainingHelper`\n",
    "            # 1.0: always sampling => `ScheduledEmbedidngTrainingHelper` is equivalent to `GreedyEmbeddingHelper`\n",
    "            # Inceasing sampling over steps => Curriculum Learning\n",
    "            \n",
    "    def add_encoder(self):\n",
    "        with tf.variable_scope('Encoder') as scope:\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.enc_Wemb = tf.get_variable('embedding',\n",
    "                    initializer=tf.random_uniform([enc_vocab_size+1, self.enc_emb_size]),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "            # [Batch_size x enc_sent_len x embedding_size]\n",
    "            enc_emb_inputs = tf.nn.embedding_lookup(\n",
    "                self.enc_Wemb, self.enc_inputs, name='emb_inputs')\n",
    "            enc_cell = self.cell(self.hidden_size)\n",
    "\n",
    "            # enc_outputs: [batch_size x enc_sent_len x embedding_size]\n",
    "            # enc_last_state: [batch_size x embedding_size]\n",
    "            enc_outputs, self.enc_last_state = tf.nn.dynamic_rnn(\n",
    "                cell=enc_cell,\n",
    "                inputs=enc_emb_inputs,\n",
    "                sequence_length=self.enc_sequence_length,\n",
    "                time_major=False,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "    def add_decoder(self):\n",
    "        with tf.variable_scope('Decoder') as scope:\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.dec_Wemb = tf.get_variable('embedding',\n",
    "                    initializer=tf.random_uniform([dec_vocab_size+2, self.dec_emb_size]),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "            dec_cell = self.cell(self.hidden_size)\n",
    "\n",
    "            # output projection (replacing `OutputProjectionWrapper`)\n",
    "            output_layer = Dense(dec_vocab_size+2, name='output_projection')\n",
    "            \n",
    "            if self.mode == 'training':\n",
    "\n",
    "                # maximum unrollings in current batch = max(dec_sent_len) + 1(GO symbol)\n",
    "                max_dec_len = tf.reduce_max(self.dec_sequence_length+1, name='max_dec_len')\n",
    "\n",
    "                dec_emb_inputs = tf.nn.embedding_lookup(\n",
    "                    self.dec_Wemb, self.dec_inputs, name='emb_inputs')\n",
    "\n",
    "                training_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs=dec_emb_inputs,\n",
    "                    sequence_length=self.dec_sequence_length+1,\n",
    "                    embedding=self.dec_Wemb,\n",
    "                    sampling_probability=self.sampling_probability,\n",
    "                    time_major=False,\n",
    "                    name='training_helper')                \n",
    "\n",
    "                training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=training_helper,\n",
    "                    initial_state=self.enc_last_state,\n",
    "                    output_layer=output_layer) \n",
    "\n",
    "                train_dec_outputs, train_dec_last_state = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    training_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=max_dec_len)\n",
    "                \n",
    "                # dec_outputs: collections.namedtuple(rnn_outputs, sample_id)\n",
    "                # dec_outputs.rnn_output: [batch_size x max(dec_sequence_length) x dec_vocab_size+2], tf.float32\n",
    "                # dec_outputs.sample_id [batch_size], tf.int32\n",
    "                \n",
    "                # logits: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                logits = tf.identity(train_dec_outputs.rnn_output, name='logits')\n",
    "                \n",
    "                # targets: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                targets = tf.slice(self.dec_inputs, [0, 0], [-1, max_dec_len], 'targets')\n",
    "                \n",
    "                # masks: [batch_size x max_dec_len]\n",
    "                # => ignore outputs after `dec_senquence_length+1` when calculating loss\n",
    "                masks = tf.sequence_mask(self.dec_sequence_length+1, max_dec_len, dtype=tf.float32, name='masks')\n",
    "                \n",
    "                # Control loss dimensions with `average_across_timesteps` and `average_across_batch`\n",
    "                # internal: `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "                self.batch_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits=logits,\n",
    "                    targets=targets,\n",
    "                    weights=masks,\n",
    "                    name='batch_loss')\n",
    "                \n",
    "                # prediction sample for validation\n",
    "                # some sample_id are overwritten with '-1's\n",
    "                self.valid_predictions = tf.argmax(logits, axis=2, name='valid_predictions')\n",
    "                \n",
    "                # List of training variables\n",
    "                # self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            \n",
    "            elif self.mode == 'inference':\n",
    "\n",
    "                batch_size = tf.shape(self.enc_inputs)[0:1]\n",
    "                start_tokens = tf.zeros(batch_size, dtype=tf.int32)\n",
    "\n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding=self.dec_Wemb,\n",
    "                    start_tokens=start_tokens,\n",
    "                    end_token=1)\n",
    "                \n",
    "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=inference_helper,\n",
    "                    initial_state=self.enc_last_state,\n",
    "                    output_layer=output_layer)\n",
    "                \n",
    "                infer_dec_outputs, infer_dec_last_state = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=dec_sentence_length)\n",
    "                \n",
    "                # [batch_size x dec_sentence_length], tf.int32\n",
    "                self.predictions = tf.identity(infer_dec_outputs.sample_id, name='predictions')\n",
    "                # equivalent to tf.argmax(infer_dec_outputs.rnn_output, axis=2, name='predictions')\n",
    "\n",
    "                # List of training variables\n",
    "                # self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        \n",
    "    def add_training_op(self):\n",
    "        self.training_op = self.optimizer(self.learning_rate, name='training_op').minimize(self.batch_loss)\n",
    "        \n",
    "    def save(self, sess, var_list=None, save_path=None):\n",
    "        print(f'Saving model at {save_path}')\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.save(sess, save_path, write_meta_graph=False)\n",
    "        \n",
    "    def restore(self, sess, var_list=None, ckpt_path=None):\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        self.restorer = tf.train.Saver(var_list)\n",
    "        self.restorer.restore(sess, ckpt_path)\n",
    "        print('Restore Finished!')\n",
    "        \n",
    "    def summary(self):\n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir=self.ckpt_dir,\n",
    "            graph=tf.get_default_graph())\n",
    "        \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_encoder()\n",
    "        self.add_decoder()\n",
    "        \n",
    "    def train(self, sess, data, from_scratch=False, load_ckpt=None, save_path=None):\n",
    "        \n",
    "        # Restore Checkpoint\n",
    "        if from_scratch is False and os.path.isfile(load_ckpt):\n",
    "            self.restore(sess, load_ckpt)\n",
    "    \n",
    "        # Add Optimizer to current graph\n",
    "        self.add_training_op()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        input_batches, target_batches = data\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in tqdm(range(self.n_epoch)):\n",
    "\n",
    "            all_preds = []\n",
    "            epoch_loss = 0\n",
    "            for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "                input_batch_tokens = []\n",
    "                target_batch_tokens = []\n",
    "                input_batch_sent_lens = []\n",
    "                target_batch_sent_lens = []\n",
    "\n",
    "                for input_sent in input_batch:\n",
    "                    tokens, sent_len = sent2idx(input_sent)\n",
    "                    input_batch_tokens.append(tokens)\n",
    "                    input_batch_sent_lens.append(sent_len)\n",
    "\n",
    "                for target_sent in target_batch:\n",
    "                    tokens, sent_len = sent2idx(target_sent,\n",
    "                                 vocab=dec_vocab,\n",
    "                                 max_sentence_length=dec_sentence_length,\n",
    "                                 is_target=True)\n",
    "                    target_batch_tokens.append(tokens)\n",
    "                    target_batch_sent_lens.append(sent_len)\n",
    "       \n",
    "                # Evaluate 3 ops in the graph\n",
    "                # => valid_predictions, loss, training_op(optimzier)\n",
    "                batch_valid_preds, batch_loss, _ = sess.run(\n",
    "                    [self.valid_predictions, self.batch_loss, self.training_op],\n",
    "                    feed_dict={\n",
    "                        self.enc_inputs: input_batch_tokens,\n",
    "                        self.enc_sequence_length: input_batch_sent_lens,\n",
    "                        self.dec_inputs: target_batch_tokens,\n",
    "                        self.dec_sequence_length: target_batch_sent_lens,\n",
    "                        self.sampling_probability: self.sampling_probability_list[epoch]\n",
    "                    }\n",
    "                )\n",
    "                # loss_history.append(batch_loss)\n",
    "                epoch_loss += batch_loss\n",
    "                all_preds.append(batch_valid_preds)\n",
    "                \n",
    "            loss_history.append(epoch_loss)\n",
    "                        \n",
    "            # Logging every 400 epochs\n",
    "            if epoch % 400 == 0:\n",
    "                print('Epoch', epoch)\n",
    "                print(f'Sampling probability: {self.sampling_probability_list[epoch]:.3f}')\n",
    "                for input_batch, target_batch, batch_preds in zip(input_batches, target_batches, all_preds):\n",
    "                    for input_sent, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "                        print(f'\\tInput: {input_sent}')\n",
    "                        print(f'\\tPrediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab))\n",
    "                        print(f'\\tTarget: {target_sent}\\n')\n",
    "                print(f'\\tepoch loss: {epoch_loss:.2f}\\n')\n",
    "                \n",
    "        if save_path:\n",
    "            self.save(sess, save_path=save_path)\n",
    "\n",
    "        return loss_history\n",
    "    \n",
    "    def inference(self, sess, data, load_ckpt):\n",
    "\n",
    "        self.restore(sess, ckpt_path=load_ckpt)\n",
    "                \n",
    "        input_batch, target_batch = data\n",
    "        \n",
    "        batch_preds = []\n",
    "        batch_tokens = []\n",
    "        batch_sent_lens = []\n",
    "\n",
    "        for input_sent in input_batch:\n",
    "            tokens, sent_len = sent2idx(input_sent)\n",
    "            batch_tokens.append(tokens)\n",
    "            batch_sent_lens.append(sent_len)\n",
    "            \n",
    "        batch_preds = sess.run(\n",
    "            self.predictions,\n",
    "            feed_dict={\n",
    "                self.enc_inputs: batch_tokens,\n",
    "                self.enc_sequence_length: batch_sent_lens,\n",
    "            })\n",
    "\n",
    "        for input_sent, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "            print('Input:', input_sent)\n",
    "            print('Prediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab))\n",
    "            print('Target:', target_sent, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if models are sucessufully built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='training')\n",
    "model.build()\n",
    "# model.summary()\n",
    "print('Training model built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='inference')\n",
    "model.build()\n",
    "# model.summary()\n",
    "print('Inference model built!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmin/anaconda/envs/mldemo/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "  0%|          | 3/801 [00:00<01:54,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Sampling probability: 0.000\n",
      "\tInput: Hi What is your name?\n",
      "\tPrediction: too too too please please please _GO\n",
      "\tTarget: Hi this is Jaemin.\n",
      "\n",
      "\tInput: Nice to meet you!\n",
      "\tPrediction: this I I please please please please\n",
      "\tTarget: Nice to meet you too!\n",
      "\n",
      "\tInput: Which programming language do you use?\n",
      "\tPrediction: this this I please please\n",
      "\tTarget: I like Python.\n",
      "\n",
      "\tInput: See you later.\n",
      "\tPrediction: this this industrial please _GO\n",
      "\tTarget: Bye Bye.\n",
      "\n",
      "\tInput: Where do you live?\n",
      "\tPrediction: this I I industrial industrial please please please please\n",
      "\tTarget: I live in Seoul, South Korea.\n",
      "\n",
      "\tInput: What is your major?\n",
      "\tPrediction: this too too I I please _GO _GO _GO\n",
      "\tTarget: I study industrial engineering.\n",
      "\n",
      "\tInput: What do you want to drink?\n",
      "\tPrediction: this this I I\n",
      "\tTarget: Beer please!\n",
      "\n",
      "\tInput: What is your favorite beer?\n",
      "\tPrediction: this this this please\n",
      "\tTarget: Leffe brown!\n",
      "\n",
      "\tepoch loss: 13.94\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 402/801 [00:40<00:40,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Sampling probability: 0.500\n",
      "\tInput: Hi What is your name?\n",
      "\tPrediction: _GO Hi this is Jaemin . _GO\n",
      "\tTarget: Hi this is Jaemin.\n",
      "\n",
      "\tInput: Nice to meet you!\n",
      "\tPrediction: _GO Nice to meet you too !\n",
      "\tTarget: Nice to meet you too!\n",
      "\n",
      "\tInput: Which programming language do you use?\n",
      "\tPrediction: _GO I like Python .\n",
      "\tTarget: I like Python.\n",
      "\n",
      "\tInput: See you later.\n",
      "\tPrediction: _GO Bye Bye . _GO\n",
      "\tTarget: Bye Bye.\n",
      "\n",
      "\tInput: Where do you live?\n",
      "\tPrediction: _GO I live in Seoul , South Korea .\n",
      "\tTarget: I live in Seoul, South Korea.\n",
      "\n",
      "\tInput: What is your major?\n",
      "\tPrediction: _GO I study industrial engineering . _GO _GO _GO\n",
      "\tTarget: I study industrial engineering.\n",
      "\n",
      "\tInput: What do you want to drink?\n",
      "\tPrediction: _GO Beer please !\n",
      "\tTarget: Beer please!\n",
      "\n",
      "\tInput: What is your favorite beer?\n",
      "\tPrediction: _GO Leffe brown !\n",
      "\tTarget: Leffe brown!\n",
      "\n",
      "\tepoch loss: 0.08\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 801/801 [01:19<00:00, 10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800\n",
      "Sampling probability: 1.000\n",
      "\tInput: Hi What is your name?\n",
      "\tPrediction: _GO Hi this is Jaemin . _GO\n",
      "\tTarget: Hi this is Jaemin.\n",
      "\n",
      "\tInput: Nice to meet you!\n",
      "\tPrediction: _GO Nice to meet you too !\n",
      "\tTarget: Nice to meet you too!\n",
      "\n",
      "\tInput: Which programming language do you use?\n",
      "\tPrediction: _GO I like Python .\n",
      "\tTarget: I like Python.\n",
      "\n",
      "\tInput: See you later.\n",
      "\tPrediction: _GO Bye Bye . _GO\n",
      "\tTarget: Bye Bye.\n",
      "\n",
      "\tInput: Where do you live?\n",
      "\tPrediction: _GO I live in Seoul , South Korea .\n",
      "\tTarget: I live in Seoul, South Korea.\n",
      "\n",
      "\tInput: What is your major?\n",
      "\tPrediction: _GO I study industrial engineering . _GO _GO _GO\n",
      "\tTarget: I study industrial engineering.\n",
      "\n",
      "\tInput: What do you want to drink?\n",
      "\tPrediction: _GO Beer please !\n",
      "\tTarget: Beer please!\n",
      "\n",
      "\tInput: What is your favorite beer?\n",
      "\tPrediction: _GO Leffe brown !\n",
      "\tTarget: Leffe brown!\n",
      "\n",
      "\tepoch loss: 0.00\n",
      "\n",
      "Saving model at ./ckpt_dir/epoch_801_sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()     \n",
    "with tf.Session() as sess:\n",
    "    config = DemoConfig()\n",
    "    model = Seq2SeqModel(config, mode='training')\n",
    "    model.build()\n",
    "    data = (input_batches, target_batches)\n",
    "    loss_history = model.train(sess, data, from_scratch=True, save_path=model.ckpt_dir+f'epoch_{model.n_epoch}_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAJcCAYAAABwj4S5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEAiwiMAAAAAAAAk+v9TQllJa2KDKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114c95390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(range(model.n_epoch), loss_history)\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Global step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/epoch_801_sampling\n",
      "Restore Finished!\n",
      "Input: Hi What is your name?\n",
      "Prediction: _GO Hi this is Jaemin . . . . .\n",
      "Target: Hi this is Jaemin. \n",
      "\n",
      "Input: Nice to meet you!\n",
      "Prediction: _GO Nice to meet you too ! ! ! !\n",
      "Target: Nice to meet you too! \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/epoch_801_sampling\n",
      "Restore Finished!\n",
      "Input: Which programming language do you use?\n",
      "Prediction: _GO I like Python . . . . . .\n",
      "Target: I like Python. \n",
      "\n",
      "Input: See you later.\n",
      "Prediction: _GO Bye Bye . . . . . . .\n",
      "Target: Bye Bye. \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/epoch_801_sampling\n",
      "Restore Finished!\n",
      "Input: Where do you live?\n",
      "Prediction: _GO I live in Seoul , South Korea . .\n",
      "Target: I live in Seoul, South Korea. \n",
      "\n",
      "Input: What is your major?\n",
      "Prediction: _GO I study industrial engineering . . . . .\n",
      "Target: I study industrial engineering. \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/epoch_801_sampling\n",
      "Restore Finished!\n",
      "Input: What do you want to drink?\n",
      "Prediction: _GO Beer please ! ! ! . . . .\n",
      "Target: Beer please! \n",
      "\n",
      "Input: What is your favorite beer?\n",
      "Prediction: _GO Leffe brown ! ! . . . . .\n",
      "Target: Leffe brown! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    config = DemoConfig()\n",
    "    model = Seq2SeqModel(config, mode='inference')\n",
    "    model.build()\n",
    "    for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "        data = (input_batch, target_batch)\n",
    "        model.inference(sess, data, load_ckpt=model.ckpt_dir+f'epoch_{model.n_epoch}_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better performance than without scheduled sampling!\n",
    "- A word of caution: http://www.inference.vc/scheduled-sampling-for-rnns-scoring-rule-interpretation/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mldemo]",
   "language": "python",
   "name": "conda-env-mldemo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
